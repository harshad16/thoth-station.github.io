
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Monte Carlo Tree Search (MCTS) &#8212; Thoth Adviser 0.19.1 documentation</title>
    <link rel="stylesheet" href="/assets/nameko.css" type="text/css" />
    <link rel="stylesheet" href="/assets/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="/assets/documentation_options.js"></script>
    <script type="text/javascript" src="/assets/jquery.js"></script>
    <script type="text/javascript" src="/assets/underscore.js"></script>
    <script type="text/javascript" src="/assets/doctools.js"></script>
    <script type="text/javascript" src="/assets/language_data.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Temporal Difference learning (TD-learning)" href="temporal_difference_learning.html" />
    <link rel="prev" title="Reinforcement learning driven resolution process" href="reinforcement_learning.html" />
   
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic|Lora:400' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

  </head><body>
  
  

    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="temporal_difference_learning.html" title="Temporal Difference learning (TD-learning)"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="reinforcement_learning.html" title="Reinforcement learning driven resolution process"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Thoth Adviser 0.19.1 documentation</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="monte-carlo-tree-search-mcts">
<span id="mcts"></span><h1>Monte Carlo Tree Search (MCTS)<a class="headerlink" href="#monte-carlo-tree-search-mcts" title="Permalink to this headline">¶</a></h1>
<p>This method is also many times referred as “Monte Carlo learning”.</p>
<p>See <a class="reference external" href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Wikipedia for a brief intro to MCTS</a>.</p>
<p>The core idea lies in creating “trajectories” into dependency graph and learn
policy for which the software stacks should be resolved based on the reward
obtained.</p>
<a class="reference external image-reference" href="/assets/mcts.gif"><img alt="Resolving software stacks with Monte Carlo Tree Search and policy learnt." src="/assets/mcts.gif" /></a>
<p>The figures below show a resolution that is guided using an MCTS predictor.</p>
<p>The very first figure shows exploration and exploitation phases balancing using
annealing schedule. As can be seen, the exploitation phase finishes roughly in
iteration 30000 when the acceptance probability for expanding a neighbour
candidate drops to 0 and only the highest rated candidates are expanded
(exploitation).</p>
<a class="reference external image-reference" href="/assets/mcts_predictor.png"><img alt="Progress made during MCTS guided resolution." src="/assets/mcts_predictor.png" /></a>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The exploration and exploitation phase is balanced using an annealing
schedule. The balance factor can be parametrized using
<code class="docutils literal notranslate"><span class="pre">temperature_coefficient</span></code> parameter that can be supplied to the predictor.
see <a class="reference internal" href="reinforcement_learning.html#rl-balancing"><span class="std std-ref">reinforcement learning intro section</span></a> for more info.</p>
</div>
<p>The second figure shows how software stacks produced by resolver increased they
score during the exploitation phase until circa 30000 iteration. This
corresponds to start of the exploitation phase. No better software stack was
resolved since then.</p>
<a class="reference external image-reference" href="/assets/mcts_resolver.png"><img alt="Statistics from the resolver during MCTS guided resolution." src="/assets/mcts_resolver.png" /></a>
<p>The last figure shows size of the beam keeping unresolved (partially resolved)
states. The max score of partially resolved states increase until iteration
30000 (exploration phase) and then the max score for partially resolved states
relatively stabilizes. It’s also good to spot how the beam size behaves. Once
the exploration phase finishes, beam size does not increase that drastically.</p>
<a class="reference external image-reference" href="/assets/mcts_beam.png"><img alt="Beam information during the MCTS guided resolution." src="/assets/mcts_beam.png" /></a>
<p>It’s also worth to point out the dataset used when creating plots was made out
of sparse scores.</p>
<p>See <a class="reference external" href="https://github.com/thoth-station/notebooks/blob/master/notebooks/development/Gradient-free%20reinforcement%20learning%20predictors.ipynb">this notebook that shows the resolution process using MCTS predictor</a>.</p>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation index</a><ul>
      <li>Previous: <a href="reinforcement_learning.html" title="previous chapter">Reinforcement learning driven resolution process</a></li>
      <li>Next: <a href="temporal_difference_learning.html" title="next chapter">Temporal Difference learning (TD-learning)</a></li>
  </ul></li>
</ul>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../sources/predictors/mcts.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>

  
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123174547-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-123174547-1');
    </script>

  </body>
</html>