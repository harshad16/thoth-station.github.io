
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Reinforcement learning driven resolution process &#8212; Thoth Adviser 0.19.1 documentation</title>
    <link rel="stylesheet" href="/assets/nameko.css" type="text/css" />
    <link rel="stylesheet" href="/assets/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="/assets/documentation_options.js"></script>
    <script type="text/javascript" src="/assets/jquery.js"></script>
    <script type="text/javascript" src="/assets/underscore.js"></script>
    <script type="text/javascript" src="/assets/doctools.js"></script>
    <script type="text/javascript" src="/assets/language_data.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Monte Carlo Tree Search (MCTS)" href="mcts.html" />
    <link rel="prev" title="Adaptive simulated annealing" href="annealing.html" />
   
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic|Lora:400' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

  </head><body>
  
  

    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="mcts.html" title="Monte Carlo Tree Search (MCTS)"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="annealing.html" title="Adaptive simulated annealing"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Thoth Adviser 0.19.1 documentation</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="reinforcement-learning-driven-resolution-process">
<span id="rl"></span><h1>Reinforcement learning driven resolution process<a class="headerlink" href="#reinforcement-learning-driven-resolution-process" title="Permalink to this headline">¶</a></h1>
<p>As described in the introductory section, the whole resolution can be treated
as a <a class="reference internal" href="../introduction.html#introduction-rl"><span class="std std-ref">Markov Decision Process (MDP)</span></a> (see <a class="reference external" href="https://en.wikipedia.org/wiki/Markov_decision_process">MDP on
Wikipedia</a>). Treating
the resolution process this way gave birth to implementations of predictors
that are based on reinforcement learning (RL). The upcoming sections will
discuss gradient-free methods:</p>
<ul class="simple">
<li><p><a class="reference internal" href="mcts.html#mcts"><span class="std std-ref">Monte Carlo Tree Search</span></a> (also known as Monte Carlo learning)</p></li>
<li><p><a class="reference internal" href="temporal_difference_learning.html#temporal-difference-learning"><span class="std std-ref">Temporal Difference learning</span></a> and its n-step variation</p></li>
</ul>
<p>MCTS based predictor as well as TD learning based predictor share core ideas
and concepts. As both RL algorithms expect an opponent in their basic
implementation, these algorithms had to be additionally adjusted to work well
in a resolution process. The main adjustment lies in balancing exploration and
exploitation as there is no “opponent” to play against (formulas like UCB1
cannot be directly applied).</p>
<div class="section" id="balancing-exploration-and-exploitation-in-rl-driven-resolution-process">
<span id="rl-balancing"></span><h2>Balancing exploration and exploitation in RL driven resolution process<a class="headerlink" href="#balancing-exploration-and-exploitation-in-rl-driven-resolution-process" title="Permalink to this headline">¶</a></h2>
<p>Exploring the whole state space of all the possible software stacks can be time
and computationally intensive task. Given the size of software stacks for any
real world applications, it is often nearly impossible to explore the whole
state space in a reasonable time. Resolving all the stacks can result in
billions of combinations that are additionally scored.</p>
<p>In these cases, the real opponent to play against is time. The idea of
temperature function from <a class="reference internal" href="annealing.html#annealing"><span class="std std-ref">adaptive simulated annealing</span></a> was
used. The temperature function balances exploration and exploitation. If
the temperature drops to 0, only exploitation is done. Otherwise, exploration is
done with a certain probability of exploration given the current temperature
(the lower temperature is, the lower probability of exploration). The
temperature can take into account number of software stacks resolved so far,
number of software stacks to resolve, number of resolver rounds and other
internal attributes of resolver.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The temperature function can be plotted when <code class="docutils literal notranslate"><span class="pre">--plot</span></code> option is supplied to
an adviser run.</p>
</div>
<p>See <a class="reference external" href="https://github.com/thoth-station/notebooks/blob/master/notebooks/development/Gradient-free%20reinforcement%20learning%20predictors.ipynb">this notebook that shows the resolution process using TD-learning
predictor</a>.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Reinforcement learning driven resolution process</a><ul>
<li><a class="reference internal" href="#balancing-exploration-and-exploitation-in-rl-driven-resolution-process">Balancing exploration and exploitation in RL driven resolution process</a></li>
</ul>
</li>
</ul>
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation index</a><ul>
      <li>Previous: <a href="annealing.html" title="previous chapter">Adaptive simulated annealing</a></li>
      <li>Next: <a href="mcts.html" title="next chapter">Monte Carlo Tree Search (MCTS)</a></li>
  </ul></li>
</ul>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../sources/predictors/reinforcement_learning.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>

  
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123174547-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-123174547-1');
    </script>

  </body>
</html>