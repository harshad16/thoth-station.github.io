
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Using deep reinforcement learning in predictor &#8212; Thoth Adviser 0.19.1 documentation</title>
    <link rel="stylesheet" href="/assets/nameko.css" type="text/css" />
    <link rel="stylesheet" href="/assets/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="/assets/documentation_options.js"></script>
    <script type="text/javascript" src="/assets/jquery.js"></script>
    <script type="text/javascript" src="/assets/underscore.js"></script>
    <script type="text/javascript" src="/assets/doctools.js"></script>
    <script type="text/javascript" src="/assets/language_data.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Package Combinations Predictor" href="package_combinations.html" />
    <link rel="prev" title="Temporal Difference learning (TD-learning)" href="temporal_difference_learning.html" />
   
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic|Lora:400' rel='stylesheet' type='text/css'>
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

  </head><body>
  
  

    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="package_combinations.html" title="Package Combinations Predictor"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="temporal_difference_learning.html" title="Temporal Difference learning (TD-learning)"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../index.html">Thoth Adviser 0.19.1 documentation</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="using-deep-reinforcement-learning-in-predictor">
<span id="neural-network"></span><h1>Using deep reinforcement learning in predictor<a class="headerlink" href="#using-deep-reinforcement-learning-in-predictor" title="Permalink to this headline">¶</a></h1>
<p>Previous sections discuss about Markov decision process (MDP) and reinforcement
learning based dependency resolution that uses gradient-free methods. Naturally,
gradient based methods will be discussed next.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Gradient based methods are not used during the dependency resolution. This
part of the documentation serves as a note to future ourselves.</p>
</div>
<p>The upcoming video demonstrates a spike that was made for a gradient based dependency resolution.</p>
<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
    <iframe src="https://www.youtube.com/embed/XtuWGex0ync" frameborder="0" allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>
</div><p>In a nutshell, training a neural network during the resolution process seems to
be tricky especially considering how the state space behaves. As a set of
actions that are possible from a state are not constant and vary from state to
state, it is required to create a set of trajectories from the dependency graph
that basically sample how the resolution can look like. Then, it’s possible to
use these trajectories to obtain a set of possible actions - packages and
package versions that can be resolved during the resolution process.
Trajectories are stored in a replay buffer as they keep information about
actions taken and reward signals obtained. The replay buffer is used to train a
neural network. Note the replay buffer needs to be shuffled not to provide
samples that depend on each other as an input to the neural network training.</p>
<p>The video proposes a neural network for regression. This architecture was made
during the experiment to reduce number of trainables by encoding the state and
actions on the input. It’s also possible to use classification as well though.</p>
<p>Note the neural network needs to be created and trained during the resolution
process (considering the current case) and the input vector size as well as its
overall size is not known (opens “neural network meta-programming question
here”). Thus the neural network cannot be fine-tuned and subsequently pushed to
production.</p>
<p>The video above also discusses a use case where the neural network could be
more suitable than TD-learning or MCTS methods. If the knowledge base consists
of a lot of causal data (e.g. a reward signal obtained in the resolution
process depends on the presence of package X and Y while resolving package Z)
the neural network can learn this patterns and guide the resolution process to
better solutions. Otherwise, gradient-free methods sound like a better solution
(consider time and number of stacks that can be scored if gradient-free methods
are used in comparision to resource hungry and time consuming neural network
training).</p>
<p>If such causal data would be available in the future, the neural network does
not need to be trained during the resolution process as discussed. As such
causal data are known beforehand, the neural network (or other trainable
entity) can be trained prior to the resolution process and can be used on
dependency sub-graphs where it would guide the resolution process.  In other
words, the neural network would be trained on the causal data and would be used
during the resolution process as part of another predictor if the dependency
sub-graph for which it was trained is spotted during the resolution process. In
other cases, gradient-free methods can be used.</p>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation index</a><ul>
      <li>Previous: <a href="temporal_difference_learning.html" title="previous chapter">Temporal Difference learning (TD-learning)</a></li>
      <li>Next: <a href="package_combinations.html" title="next chapter">Package Combinations Predictor</a></li>
  </ul></li>
</ul>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../sources/predictors/neural_network.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>

  
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123174547-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-123174547-1');
    </script>

  </body>
</html>